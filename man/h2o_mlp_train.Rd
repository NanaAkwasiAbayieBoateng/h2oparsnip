% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mlp.R
\name{h2o_mlp_train}
\alias{h2o_mlp_train}
\title{Wrapper for training a h2o.deeplearning model as part of a parsnip `mlp`
h2o engine}
\usage{
h2o_mlp_train(
  formula,
  data,
  l2 = 0,
  hidden_dropout_ratios = 0,
  hidden = 100,
  epochs = 10,
  activation = "Rectifier",
  stopping_rounds = 0,
  validation = 0,
  ...
)
}
\arguments{
\item{formula}{formula}

\item{data}{data.frame of training data}

\item{l2}{numeric, l2 regulation parameter, default = 0}

\item{hidden_dropout_ratios}{dropout ratio for a single hidden layer (default
= 0)}

\item{hidden}{integer, number of neurons in the hidden layer (default = c(200, 200))}

\item{epochs}{integer, number of epochs (default = 10)}

\item{activation}{character, activation function. Must be one of: "Tanh",
"TanhWithDropout", "Rectifier", "RectifierWithDropout", "Maxout",
"MaxoutWithDropout". Defaults to "Rectifier. If `hidden_dropout_ratios` > 0
then the equivalent activation function with dropout is used.}

\item{stopping_rounds}{An integer specifying the number of training
iterations without improvement before stopping. If `stopping_rounds = 0`
(the default) then early stopping is disabled.  If `validation` is used,
performance is base on the validation set; otherwise the training set is
used.}

\item{validation}{A positive number. If on `[0, 1)` the value, `validation`
is a random proportion of data in `x` and `y` that are used for performance
assessment and potential early stopping. If 1 or greater, it is the _number_
of training set samples use for these purposes.}

\item{...}{other arguments not currently used}
}
\value{
evaluated h2o model call
}
\description{
Wrapper for training a h2o.deeplearning model as part of a parsnip `mlp`
h2o engine
}
